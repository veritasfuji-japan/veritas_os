#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VERITAS TrustLog Merger - å®Œå…¨ç‰ˆ

è¤‡æ•°ã® TrustLog ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒžãƒ¼ã‚¸ã—ã¦é‡è¤‡ã‚’é™¤åŽ»ã—ã€
created_at ã§ã‚½ãƒ¼ãƒˆã—ãŸã†ãˆã§ sha256 / sha256_prev ã‚’å†è¨ˆç®—ã—ã¾ã™ã€‚

- request_id ã‚’ã‚­ãƒ¼ã«é‡è¤‡æŽ’é™¤ï¼ˆåŒä¸€IDã¯ created_at ãŒæ–°ã—ã„æ–¹ã‚’æŽ¡ç”¨ï¼‰
- created_at / timestamp ãŒãªã„å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚‚ãã®ã¾ã¾å–ã‚Šè¾¼ã¿ï¼ˆã‚½ãƒ¼ãƒˆæ™‚ã¯ç©ºæ–‡å­—æ‰±ã„ï¼‰
- å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã¯ JSONL (1 è¡Œ = 1 JSON)

Usage (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‚’ä½¿ã†):
    python merge_trust_logs.py

Usage (å‡ºåŠ›å…ˆã‚’æŒ‡å®š):
    python merge_trust_logs.py --out /path/to/trust_log_merged.jsonl
"""

from __future__ import annotations

import argparse
import json
import hashlib
from pathlib import Path
from typing import Any, Dict, List, Optional

# ===== ãƒ‘ã‚¹è¨­å®š =====

# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«: veritas_os/scripts/merge_trust_logs.py ã‚’æƒ³å®š
# parents[0] = .../veritas_os/scripts
# parents[1] = .../veritas_os
# parents[2] = .../repo_root
REPO_ROOT = Path(__file__).resolve().parents[2]
TOP_SCRIPTS_DIR = REPO_ROOT / "scripts"
TOP_LOGS_DIR = TOP_SCRIPTS_DIR / "logs"

PKG_ROOT = Path(__file__).resolve().parents[1]          # .../veritas_os
PKG_SCRIPTS_DIR = PKG_ROOT / "scripts"
PKG_LOGS_DIR = PKG_SCRIPTS_DIR / "logs"

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¦‹ã‚‹ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå€™è£œï¼ˆå­˜åœ¨ã™ã‚‹ã‚‚ã®ã‚’å„ªå…ˆï¼‰
LOG_DIR_CANDIDATES = [PKG_LOGS_DIR, TOP_LOGS_DIR]

def _pick_default_logs_dir() -> Path:
    for d in LOG_DIR_CANDIDATES:
        if d.exists():
            return d
    # ã©ã“ã‚‚ãªã„å ´åˆã¯ veritas_os/scripts/logs ã‚’å„ªå…ˆã§ä½œã‚‹
    PKG_LOGS_DIR.mkdir(parents=True, exist_ok=True)
    return PKG_LOGS_DIR


DEFAULT_LOGS_DIR = _pick_default_logs_dir()

# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å€™è£œ
DEFAULT_SRC_FILES = [
    DEFAULT_LOGS_DIR / "trust_log.jsonl",
    DEFAULT_LOGS_DIR / "trust_log.json",
    DEFAULT_LOGS_DIR / "trust_log_backup.jsonl",
    DEFAULT_LOGS_DIR / "trust_log_backup.json",
    PKG_SCRIPTS_DIR / "trust_log_archive.jsonl",
]

DEFAULT_OUT_PATH = DEFAULT_LOGS_DIR / "trust_log_merged.jsonl"


# ===== åŸºæœ¬ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====

def _sha256(s: str) -> str:
    """UTF-8 æ–‡å­—åˆ—ã‹ã‚‰ SHA-256 ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—"""
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


def _normalize_for_hash(entry: Dict[str, Any]) -> str:
    """
    TrustLog ã® sha256 è¨ˆç®—ç”¨ã«ã‚¨ãƒ³ãƒˆãƒªã‚’æ­£è¦åŒ–ã€‚
    - sha256 / sha256_prev ã‚’é™¤å¤–ã—ã¦ JSON åŒ–
    - sort_keys=True ã§é †åºã‚’å›ºå®š
    """
    payload = dict(entry)
    payload.pop("sha256", None)
    payload.pop("sha256_prev", None)
    return json.dumps(payload, sort_keys=True, ensure_ascii=False)


def _stable_entry_fingerprint(entry: Dict[str, Any]) -> str:
    """Return a deterministic fingerprint for entries without request_id."""
    return _sha256(_normalize_for_hash(entry))


def load_any_json(path: Path) -> List[Dict[str, Any]]:
    """
    JSON / JSONL / JSON with items[] ã‚’ã„ã„æ„Ÿã˜ã«èª­ã¿è¾¼ã‚€ã€‚

    å„ªå…ˆé †:
    1. json.loads(text) ã«æˆåŠŸã—ãŸå ´åˆ:
       - list        â†’ ãã®ã¾ã¾è¿”ã™
       - dict+items  â†’ dict["items"] ã‚’è¿”ã™
       - dict å˜ä½“   â†’ [dict] ã¨ã—ã¦è¿”ã™
    2. json.loads ã«å¤±æ•—ã—ãŸå ´åˆ:
       - JSONL ã¨ã¿ãªã—ã¦ 1 è¡Œã”ã¨ã« json.loads
    """
    try:
        text = path.read_text(encoding="utf-8").strip()
        if not text:
            return []

        # ---- ã¾ãšã¯æ™®é€šã« JSON ã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹ ----
        try:
            data = json.loads(text)
            # list ã®å ´åˆ
            if isinstance(data, list):
                return [d for d in data if isinstance(d, dict)]
            # dict ã®å ´åˆ
            if isinstance(data, dict):
                if "items" in data and isinstance(data["items"], list):
                    return [d for d in data["items"] if isinstance(d, dict)]
                # å˜ä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦æ‰±ã†
                return [data]
        except json.JSONDecodeError:
            # é€šå¸¸ JSON ã§ãªã‘ã‚Œã° JSONL ã¨ã¿ãªã™
            pass

        # ---- JSONL ãƒ¢ãƒ¼ãƒ‰ ----
        items: List[Dict[str, Any]] = []
        for line in text.splitlines():
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                if isinstance(obj, dict):
                    items.append(obj)
            except json.JSONDecodeError as e:
                print(f"âš ï¸  Skipping invalid JSON line in {path.name}: {e}")
        return items

    except FileNotFoundError:
        print(f"â­ï¸  File not found: {path}")
        return []
    except Exception as e:
        print(f"âŒ Error loading {path}: {e}")
        return []


def recompute_hash_chain(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    created_até †ã«ä¸¦ã‚“ã  TrustLog ã‚¨ãƒ³ãƒˆãƒªã«å¯¾ã—ã¦ã€
    sha256 / sha256_prev ã®ãƒã‚§ãƒ¼ãƒ³ã‚’å†è¨ˆç®—ã™ã‚‹ã€‚

    hâ‚œ = SHA256(hâ‚œâ‚‹â‚ || râ‚œ)
    """
    new_items: List[Dict[str, Any]] = []
    prev_hash: Optional[str] = None

    for item in items:
        # å…ƒã‚’å£Šã•ãªã„ã‚ˆã†ã«ã‚³ãƒ”ãƒ¼
        e = dict(item)

        # sha ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯ä¸€æ—¦æ¶ˆã—ã¦ã‹ã‚‰å†åº¦è¨ˆç®—
        e.pop("sha256", None)
        e.pop("sha256_prev", None)

        e["sha256_prev"] = prev_hash

        entry_json = _normalize_for_hash(e)
        if prev_hash:
            combined = prev_hash + entry_json
        else:
            combined = entry_json

        e["sha256"] = _sha256(combined)
        prev_hash = e["sha256"]

        new_items.append(e)

    return new_items


# ===== ãƒ¡ã‚¤ãƒ³å‡¦ç† =====

def merge_trust_logs(
    src_files: List[Path],
    out_path: Path,
    recompute_hash: bool = True,
) -> None:
    """TrustLog ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒžãƒ¼ã‚¸ã—ã¦ out_path ã« JSONL ã§å‡ºåŠ›ã€‚"""
    print("ðŸ”„ VERITAS TrustLog Merger")
    print("=" * 60)

    uniq: Dict[str, Dict[str, Any]] = {}
    total_loaded = 0

    for src in src_files:
        if not src.exists():
            print(f"â­ï¸  Skipping {src} (not found)")
            continue

        items = load_any_json(src)
        total_loaded += len(items)
        print(f"ðŸ“„ Loaded {len(items)} items from {src}")

        for item in items:
            if not isinstance(item, dict):
                continue

            # created_at ã®ç„¡ã„å¤ã„ãƒ­ã‚°ã«ã¯ timestamp ã‚’æµç”¨
            if "created_at" not in item and "timestamp" in item:
                item["created_at"] = item.get("timestamp")

            rid = item.get("request_id")
            if rid:
                # åŒã˜ request_id ã¯ created_at ãŒæ–°ã—ã„æ–¹ã‚’å„ªå…ˆ
                existing = uniq.get(rid)
                if not existing:
                    uniq[rid] = item
                else:
                    if (item.get("created_at") or "") > (existing.get("created_at") or ""):
                        uniq[rid] = item
            else:
                # request_id ãŒç„¡ã„ã‚¨ãƒ³ãƒˆãƒªã¯ created_at/timestamp ã‚’ã‚­ãƒ¼ã¨ã—ã¦æ‰±ã†
                ts_key = (
                    item.get("created_at")
                    or item.get("timestamp")
                    or _stable_entry_fingerprint(item)
                )
                existing = uniq.get(ts_key)
                if not existing:
                    uniq[ts_key] = item
                else:
                    # åŒã˜ ts_key ã®å ´åˆã¯å¾Œå‹ã¡ã«ã—ã¦ãŠã
                    uniq[ts_key] = item

    print(f"\nðŸ“Š Total loaded: {total_loaded}")
    print(f"ðŸ“Š Unique entries (by request_id / timestamp): {len(uniq)}")
    print(f"ðŸ“Š Duplicates removed: {total_loaded - len(uniq)}")

    # created_at / timestamp ã§ã‚½ãƒ¼ãƒˆ
    items = sorted(
        uniq.values(),
        key=lambda x: (x.get("created_at") or x.get("timestamp") or ""),
    )

    # å¿…è¦ã«å¿œã˜ã¦ãƒãƒƒã‚·ãƒ¥ãƒã‚§ãƒ¼ãƒ³ã‚’å†è¨ˆç®—
    if recompute_hash:
        print("ðŸ” Recomputing sha256 / sha256_prev chain ...")
        items = recompute_hash_chain(items)

    # å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    out_path.parent.mkdir(parents=True, exist_ok=True)

    # JSONL ã¨ã—ã¦æ›¸ãå‡ºã—
    with out_path.open("w", encoding="utf-8") as f:
        for item in items:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"\nâœ… Merged {len(items)} unique logs â†’ {out_path}")
    print("   Output format: JSONL (one JSON per line)")
    if recompute_hash:
        print("   Note: sha256 / sha256_prev chain has been recomputed.")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Merge VERITAS TrustLog files.")
    parser.add_argument(
        "--out",
        type=str,
        default=str(DEFAULT_OUT_PATH),
        help=f"Output JSONL path (default: {DEFAULT_OUT_PATH})",
    )
    parser.add_argument(
        "--no-rehash",
        action="store_true",
        help="Do NOT recompute sha256 / sha256_prev chain (ãŸã ã—åŽŸå‰‡ãŠã™ã™ã‚ã—ã¾ã›ã‚“)",
    )
    parser.add_argument(
        "--src",
        type=str,
        nargs="*",
        help=(
            "Source trust_log paths (override defaults). "
            "æœªæŒ‡å®šãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€™è£œã‚’ä½¿ç”¨ã€‚"
        ),
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()

    if args.src:
        src_files = [Path(p).expanduser() for p in args.src]
    else:
        src_files = DEFAULT_SRC_FILES

    out_path = Path(args.out).expanduser()
    recompute_hash = not args.no_rehash

    merge_trust_logs(src_files=src_files, out_path=out_path, recompute_hash=recompute_hash)


if __name__ == "__main__":
    main()
