#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""VERITAS Doctor (stable)
veritas_os/scripts/logs é…ä¸‹ã®ãƒ­ã‚°ã‚’è§£æã—ã¦ã€
åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã« doctor_report.json ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
"""

import os
import json
import glob
import statistics
from pathlib import Path
from datetime import datetime

# ==== ãƒ‘ã‚¹å®šç¾© ====
# doctor.py ã®å ´æ‰€: veritas_os/scripts/doctor.py ã‚’æƒ³å®š
HERE = Path(__file__).resolve().parent          # .../veritas_os/scripts
REPO_ROOT = HERE.parent                         # .../veritas_os

# ãƒ­ã‚°ç½®ãå ´ï¼ˆdecide_*.json, health_*.json ãªã©ï¼‰
LOG_DIR = HERE / "logs"                         # .../veritas_os/scripts/logs
LOG_DIR.mkdir(parents=True, exist_ok=True)

# ç›£æŸ»ç”¨ JSONLï¼ˆä»»æ„ã§ä½¿ã†å ´åˆï¼‰
TRUST_LOG_JSON = LOG_DIR / "trust_log.jsonl"
# äº’æ›ã®ãŸã‚ã®åˆ¥åï¼ˆæ˜”ã®å¤‰æ•°åï¼‰
LOG_JSONL = TRUST_LOG_JSON

# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å…ˆ
REPORT_PATH = LOG_DIR / "doctor_report.json"

# è§£æå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆJSONLå„ªå…ˆï¼é‡è¤‡é™¤å»ï¼‰
PATTERNS = [
    "decide_*.jsonl", "health_*.jsonl", "*status*.jsonl", "*.jsonl",
    "decide_*.json",  "health_*.json",  "*status*.json",
]

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¢—ã‚„ã—ã¦OKï¼‰
KW_LIST = ["äº¤æ¸‰", "å¤©æ°—", "ç–²ã‚Œ", "éŸ³æ¥½", "VERITAS"]


# ---- helpers -----------------------------------------------------------
def _iter_files() -> list[str]:
    """PATTERNS ã«ä¸€è‡´ã™ã‚‹ãƒ­ã‚°ã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’ mtime æ˜‡é †ã§è¿”ã™ï¼ˆé‡è¤‡é™¤å»ï¼‰"""
    seen, files = set(), []
    for pat in PATTERNS:
        for p in glob.glob(os.path.join(LOG_DIR, pat)):
            if p not in seen and os.path.getsize(p) > 0:
                seen.add(p)
                files.append(p)
    files.sort(key=lambda p: os.path.getmtime(p))
    return files


def _read_json_or_jsonl(path: str) -> list[dict]:
    """
    1ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¾æ›¸ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    - å…ˆé ­æ–‡å­—ã§ JSON / JSONL ã‚’åˆ¤å®š
    - å£Šã‚Œè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
    - {"items":[...]} å½¢å¼ã¯ items ã‚’å±•é–‹
    """
    items: list[dict] = []
    with open(path, "r", encoding="utf-8") as f:
        head = f.read(1)
        if not head:
            return []
        f.seek(0)

        if head == "{":  # JSON
            data = json.load(f)
            if isinstance(data, dict) and isinstance(data.get("items"), list):
                items.extend(data["items"])
            else:
                items.append(data)
        else:            # JSONL
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    items.append(json.loads(line))
                except Exception:
                    # ç ´æè¡Œã¯ç„¡è¦–ã—ã¦ç¶šè¡Œ
                    continue
    return items


def _bump_kw(counter: dict, text: str):
    for w in KW_LIST:
        if w and (w in (text or "")):
            counter[w] = counter.get(w, 0) + 1


# ---- main analyzer -----------------------------------------------------
def analyze_logs():
    files = _iter_files()

    # TRUST_LOG_JSON ãŒãªãã¦ã‚‚ã€ã¨ã‚Šã‚ãˆãšè­¦å‘Šã ã‘ã§OK
    if not files and not os.path.exists(LOG_JSONL):
        print("âš ï¸ .veritas å†…ã«è§£æå¯¾è±¡ã®ãƒ­ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    found_total  = len(files)
    parsed       = 0
    skipped_zero = 0
    skipped_bad  = 0

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    metrics = {
        "decide": {"count": 0},
        "health": {"count": 0},
        "status": {"count": 0},
        "other":  {"count": 0},
    }
    uncertainties: list[float] = []
    keywords: dict[str, int] = {}

    # ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’èµ°æŸ»
    for path in files:
        name = os.path.basename(path)
        if   name.startswith("decide_"):  cat = "decide"
        elif name.startswith("health_"):  cat = "health"
        elif "status" in name:            cat = "status"
        else:                             cat = "other"

        try:
            items = _read_json_or_jsonl(path)
        except Exception as e:
            skipped_bad += 1
            print(f"âš ï¸ {path} ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            continue

        if not items:
            skipped_zero += 1
            continue

        # ã‚¹ã‚­ãƒ¼ãƒæºã‚Œã‚’å¸åã—ã¦æŠ½å‡º
        for data in items:
            if not isinstance(data, dict):
                continue

            # query
            ctx   = data.get("context") or {}
            query = data.get("query") or ctx.get("query") or ""
            if query:
                _bump_kw(keywords, query)

            # ä¸ç¢ºå®Ÿæ€§ï¼ˆã‚ã‚Œã°ï¼‰
            chosen = (
                (data.get("response") or {}).get("chosen")
                or (data.get("result") or {}).get("chosen")
                or (data.get("decision") or {}).get("chosen")
                or data.get("chosen")
                or {}
            )
            unc = chosen.get("uncertainty", data.get("uncertainty", None))
            try:
                if unc is not None:
                    uncertainties.append(float(unc))
            except Exception:
                pass

        metrics[cat]["count"] += 1
        parsed += 1

    # ç›£æŸ» JSONLï¼ˆä»»æ„ï¼‰ã‚’èª­ã‚€ã ã‘èª­ã‚“ã§æœ€çµ‚æ™‚åˆ»ã®è£œåŠ©ã«ä½¿ã†
    last_check = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    try:
        if os.path.exists(LOG_JSONL) and os.path.getsize(LOG_JSONL) > 0:
            with open(LOG_JSONL, "r", encoding="utf-8") as f:
                tail = f.readlines()[-20:]
            for line in reversed(tail):
                try:
                    obj = json.loads(line.strip())
                    ts  = (obj.get("created_at") or "").replace("Z", "")
                    if ts:
                        last_check = ts
                        break
                except Exception:
                    continue
    except Exception:
        pass

    avg_unc = round(statistics.mean(uncertainties), 3) if uncertainties else 0.0

    result = {
        "total_files_found": found_total,
        "parsed_logs":       parsed,
        "skipped_zero":      skipped_zero,
        "skipped_badjson":   skipped_bad,
        "avg_uncertainty":   avg_unc,
        "keywords":          keywords,
        "last_check":        last_check,
        "by_category":       {k: v["count"] for k, v in metrics.items()},
        "generated_at":      datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "source_dir":        str(LOG_DIR),
    }

    os.makedirs(os.path.dirname(REPORT_PATH), exist_ok=True)
    with open(REPORT_PATH, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    # ---- console summary ------------------------------------------------
    print("\n== VERITAS Doctor Report ==")
    print("âœ“ æ¤œå‡º(ç·):", found_total)
    print("âœ“ è§£æOK :", parsed)
    print("â†ª ã‚¹ã‚­ãƒƒãƒ—: 0B=", skipped_zero, ", JSON=", skipped_bad)
    print("ğŸ¯ å¹³å‡ä¸ç¢ºå®Ÿæ€§:", avg_unc)
    print("ğŸ”‘ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦:", keywords)
    print("ğŸ“… æœ€çµ‚è¨ºæ–­æ™‚åˆ»:", last_check)
    print("ğŸ“Š ã‚«ãƒ†ã‚´ãƒªå†…è¨³:", {k: v["count"] for k, v in metrics.items()})
    print("âœ… ä¿å­˜å®Œäº†:", REPORT_PATH)


if __name__ == "__main__":
    analyze_logs()
