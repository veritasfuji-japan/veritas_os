#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""VERITAS Doctor (enhanced with TrustLog validation)
veritas_os/scripts/logs é…ä¸‹ã®ãƒ­ã‚°ã‚’è§£æã—ã¦ã€
åŒã˜ãƒ•ã‚©ãƒ«ãƒ€ã« doctor_report.json ã‚’å‡ºåŠ›ã™ã‚‹ã€‚

v2.0 æ–°æ©Ÿèƒ½:
- TrustLog ãƒãƒƒã‚·ãƒ¥ãƒã‚§ãƒ¼ãƒ³æ¤œè¨¼
- ã‚ˆã‚Šè©³ç´°ãªè¨ºæ–­æƒ…å ±
"""

import os
import json
import glob
import hashlib
import statistics
from pathlib import Path
from datetime import datetime

# ==== ãƒ‘ã‚¹å®šç¾© ====
# doctor.py ã®å ´æ‰€: veritas_os/scripts/doctor.py ã‚’æƒ³å®š
HERE = Path(__file__).resolve().parent          # .../veritas_os/scripts
REPO_ROOT = HERE.parent                         # .../veritas_os

# ãƒ­ã‚°ç½®ãå ´ï¼ˆdecide_*.json, health_*.json ãªã©ï¼‰
LOG_DIR = HERE / "logs"                         # .../veritas_os/scripts/logs
LOG_DIR.mkdir(parents=True, exist_ok=True)

# ç›£æŸ»ç”¨ JSONL
TRUST_LOG_JSON = LOG_DIR / "trust_log.jsonl"
LOG_JSONL = TRUST_LOG_JSON  # äº’æ›ã®ãŸã‚ã®åˆ¥å

# ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å…ˆ
REPORT_PATH = LOG_DIR / "doctor_report.json"

# è§£æå¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆJSONLå„ªå…ˆï¼é‡è¤‡é™¤å»ï¼‰
PATTERNS = [
    "decide_*.jsonl", "health_*.jsonl", "*status*.jsonl", "*.jsonl",
    "decide_*.json",  "health_*.json",  "*status*.json",
]

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¢—ã‚„ã—ã¦OKï¼‰
KW_LIST = ["äº¤æ¸‰", "å¤©æ°—", "ç–²ã‚Œ", "éŸ³æ¥½", "VERITAS"]

# â˜… CPU/OOMå¯¾ç­–: JSONè§£ææ™‚ã®å®‰å…¨åˆ¶é™
MAX_FILE_SIZE = 50 * 1024 * 1024      # 50MB: ã“ã‚Œã‚’è¶…ãˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¹ã‚­ãƒƒãƒ—
MAX_ITEMS_PER_FILE = 100_000          # 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®æœ€å¤§ã‚¢ã‚¤ãƒ†ãƒ æ•°
MAX_TRUSTLOG_LINES = 500_000          # TrustLogæ¤œè¨¼ã®æœ€å¤§è¡Œæ•°


# ---- TrustLog validation -------------------------------------------
def compute_hash_for_entry(prev_hash: str | None, entry: dict) -> str:
    """
    è«–æ–‡ã®å¼ã«å¾“ã£ãŸãƒãƒƒã‚·ãƒ¥è¨ˆç®—: hâ‚œ = SHA256(hâ‚œâ‚‹â‚ || râ‚œ)
    
    Args:
        prev_hash: ç›´å‰ã®ãƒãƒƒã‚·ãƒ¥å€¤ (hâ‚œâ‚‹â‚)
        entry: ç¾åœ¨ã®ã‚¨ãƒ³ãƒˆãƒª (râ‚œ)
    
    Returns:
        è¨ˆç®—ã•ã‚ŒãŸãƒãƒƒã‚·ãƒ¥å€¤ (hâ‚œ)
    """
    # ã‚¨ãƒ³ãƒˆãƒªã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€sha256ã¨sha256_prevã‚’é™¤å¤–
    payload = dict(entry)
    payload.pop("sha256", None)
    payload.pop("sha256_prev", None)
    
    # râ‚œ ã‚’ JSONåŒ–ï¼ˆã‚­ãƒ¼ã‚’ã‚½ãƒ¼ãƒˆã—ã¦ä¸€æ„æ€§ã‚’ä¿è¨¼ï¼‰
    entry_json = json.dumps(payload, sort_keys=True, ensure_ascii=False)
    
    # hâ‚œâ‚‹â‚ || râ‚œ ã‚’çµåˆ
    if prev_hash:
        combined = prev_hash + entry_json
    else:
        # æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã®å ´åˆã¯ râ‚œ ã®ã¿
        combined = entry_json
    
    # SHA-256è¨ˆç®—
    return hashlib.sha256(combined.encode("utf-8")).hexdigest()


def analyze_trustlog() -> dict:
    """
    TrustLog (trust_log.jsonl) ã®ãƒãƒƒã‚·ãƒ¥ãƒã‚§ãƒ¼ãƒ³æ¤œè¨¼
    
    Returns:
        {
            "status": "âœ… æ­£å¸¸" | "âš ï¸ ãƒã‚§ãƒ¼ãƒ³ç ´æ" | "not_found",
            "entries": int,
            "chain_valid": bool | None,
            "chain_breaks": int,
            "first_break": dict | None,
            "hash_mismatches": int,
            "first_mismatch": dict | None,
            "last_hash": str | None,
            "created_at": str | None,
        }
    """
    if not TRUST_LOG_JSON.exists():
        return {
            "status": "not_found",
            "entries": 0,
            "chain_valid": None,
            "chain_breaks": 0,
            "hash_mismatches": 0,
            "first_break": None,
            "first_mismatch": None,
            "last_hash": None,
            "created_at": None,
        }
    
    # â˜… CPU/OOMå¯¾ç­–: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
    try:
        file_size = TRUST_LOG_JSON.stat().st_size
        if file_size > MAX_FILE_SIZE:
            return {
                "status": f"skipped: file too large ({file_size} bytes)",
                "entries": 0,
                "chain_valid": None,
                "chain_breaks": 0,
                "hash_mismatches": 0,
                "first_break": None,
                "first_mismatch": None,
                "last_hash": None,
                "created_at": None,
            }
    except OSError:
        pass

    total_entries = 0
    chain_valid = True
    chain_breaks = []
    hash_mismatches = []
    prev_hash = None
    last_hash = None
    last_created_at = None
    
    try:
        with open(TRUST_LOG_JSON, "r", encoding="utf-8") as f:
            for i, line in enumerate(f, 1):
                # â˜… CPUå¯¾ç­–: è¡Œæ•°ä¸Šé™ãƒã‚§ãƒƒã‚¯
                if i > MAX_TRUSTLOG_LINES:
                    break
                line = line.strip()
                if not line:
                    continue
                
                try:
                    entry = json.loads(line)
                    total_entries += 1
                    
                    sha_prev = entry.get("sha256_prev")
                    sha_self = entry.get("sha256")
                    
                    # 1. ãƒã‚§ãƒ¼ãƒ³é€£ç¶šæ€§ã®æ¤œè¨¼
                    if sha_prev != prev_hash:
                        chain_valid = False
                        chain_breaks.append({
                            "line": i,
                            "expected_prev": prev_hash,
                            "actual_prev": sha_prev,
                            "request_id": entry.get("request_id", "unknown"),
                        })
                    
                    # 2. ãƒãƒƒã‚·ãƒ¥å€¤ã®æ¤œè¨¼ï¼ˆè«–æ–‡ã®å¼ã«å¾“ã†ï¼‰
                    calc_hash = compute_hash_for_entry(sha_prev, entry)
                    if calc_hash != sha_self:
                        chain_valid = False
                        hash_mismatches.append({
                            "line": i,
                            "expected_hash": calc_hash[:16] + "...",
                            "actual_hash": (sha_self[:16] + "...") if sha_self else None,
                            "request_id": entry.get("request_id", "unknown"),
                        })
                    
                    prev_hash = sha_self
                    last_hash = sha_self
                    
                    # æœ€çµ‚ä½œæˆæ—¥æ™‚ã‚’è¨˜éŒ²
                    if "created_at" in entry:
                        last_created_at = entry["created_at"]
                    
                except json.JSONDecodeError:
                    # ç ´æè¡Œã¯ç„¡è¦–ã—ã¦ç¶šè¡Œ
                    continue
    
    except Exception as e:
        return {
            "status": f"error: {str(e)}",
            "entries": 0,
            "chain_valid": False,
            "chain_breaks": 0,
            "hash_mismatches": 0,
            "first_break": None,
            "first_mismatch": None,
            "last_hash": None,
            "created_at": None,
        }
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
    if total_entries == 0:
        status = "empty"
    elif chain_valid:
        status = "âœ… æ­£å¸¸"
    else:
        status = "âš ï¸ ãƒã‚§ãƒ¼ãƒ³ç ´æ"
    
    return {
        "status": status,
        "entries": total_entries,
        "chain_valid": chain_valid,
        "chain_breaks": len(chain_breaks),
        "hash_mismatches": len(hash_mismatches),
        "first_break": chain_breaks[0] if chain_breaks else None,
        "first_mismatch": hash_mismatches[0] if hash_mismatches else None,
        "last_hash": last_hash[:16] + "..." if last_hash else None,
        "created_at": last_created_at,
    }


# ---- helpers -----------------------------------------------------------
def _iter_files() -> list[str]:
    """PATTERNS ã«ä¸€è‡´ã™ã‚‹ãƒ­ã‚°ã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’ mtime æ˜‡é †ã§è¿”ã™ï¼ˆé‡è¤‡é™¤å»ï¼‰"""
    seen, files = set(), []
    for pat in PATTERNS:
        for p in glob.glob(os.path.join(LOG_DIR, pat)):
            if p not in seen and os.path.getsize(p) > 0:
                seen.add(p)
                files.append(p)
    files.sort(key=lambda p: os.path.getmtime(p))
    return files


def _read_json_or_jsonl(path: str) -> list[dict]:
    """
    Read one file as JSON or JSONL and return a bounded list of records.

    This parser first attempts a full JSON parse to support the following
    patterns commonly seen in log outputs:

    * Single object: ``{"key": "value"}``
    * Wrapped items: ``{"items": [{...}, ...]}``
    * Top-level array: ``[{...}, {...}]``

    If the full parse fails, it falls back to line-delimited JSON (JSONL).
    Corrupted JSONL rows are skipped so that one bad line does not block the
    whole diagnosis.

    Security/reliability controls:

    * Skip overly large files (``MAX_FILE_SIZE``)
    * Limit parsed items per file (``MAX_ITEMS_PER_FILE``)
    """
    # â˜… CPU/OOMå¯¾ç­–: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
    try:
        file_size = os.path.getsize(path)
        if file_size > MAX_FILE_SIZE:
            print(f"âš ï¸ {path} is too large ({file_size} bytes), skipping")
            return []
    except OSError:
        return []

    items: list[dict] = []

    def _bounded_extend(values: list) -> None:
        """Extend ``items`` up to ``MAX_ITEMS_PER_FILE`` entries."""
        remaining = MAX_ITEMS_PER_FILE - len(items)
        if remaining <= 0:
            return
        items.extend(values[:remaining])

    with open(path, "r", encoding="utf-8") as f:
        content = f.read()
        if not content:
            return []

    # Prefer JSON first (handles leading whitespace safely).
    try:
        data = json.loads(content)
    except json.JSONDecodeError:
        data = None

    if data is not None:
        if isinstance(data, dict) and isinstance(data.get("items"), list):
            _bounded_extend(data["items"])
        elif isinstance(data, list):
            _bounded_extend(data)
        else:
            _bounded_extend([data])
        return items

    # Fallback: JSONL mode
    for raw_line in content.splitlines():
        line = raw_line.strip()
        if not line:
            continue
        try:
            items.append(json.loads(line))
        except json.JSONDecodeError:
            # ç ´æè¡Œã¯ç„¡è¦–ã—ã¦ç¶šè¡Œ
            continue
        # â˜… CPU/OOMå¯¾ç­–: ã‚¢ã‚¤ãƒ†ãƒ æ•°ã®ä¸Šé™ãƒã‚§ãƒƒã‚¯
        if len(items) >= MAX_ITEMS_PER_FILE:
            break
    return items


def _bump_kw(counter: dict, text: str):
    for w in KW_LIST:
        if w and (w in (text or "")):
            counter[w] = counter.get(w, 0) + 1


# ---- main analyzer -----------------------------------------------------
def analyze_logs():
    files = _iter_files()

    # TRUST_LOG_JSON ãŒãªãã¦ã‚‚ã€ã¨ã‚Šã‚ãˆãšè­¦å‘Šã ã‘ã§OK
    if not files and not os.path.exists(LOG_JSONL):
        print("âš ï¸ scripts/logs å†…ã«è§£æå¯¾è±¡ã®ãƒ­ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    found_total  = len(files)
    parsed       = 0
    skipped_zero = 0
    skipped_bad  = 0

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    metrics = {
        "decide": {"count": 0},
        "health": {"count": 0},
        "status": {"count": 0},
        "other":  {"count": 0},
    }
    uncertainties: list[float] = []
    keywords: dict[str, int] = {}

    # ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’èµ°æŸ»
    for path in files:
        name = os.path.basename(path)
        if name.startswith("decide_"):
            cat = "decide"
        elif name.startswith("health_"):
            cat = "health"
        elif "status" in name:
            cat = "status"
        else:
            cat = "other"

        try:
            items = _read_json_or_jsonl(path)
        except Exception as e:
            skipped_bad += 1
            print(f"âš ï¸ {path} ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
            continue

        if not items:
            skipped_zero += 1
            continue

        # ã‚¹ã‚­ãƒ¼ãƒæºã‚Œã‚’å¸åã—ã¦æŠ½å‡º
        for data in items:
            if not isinstance(data, dict):
                continue

            # query
            ctx   = data.get("context") or {}
            query = data.get("query") or ctx.get("query") or ""
            if query:
                _bump_kw(keywords, query)

            # ä¸ç¢ºå®Ÿæ€§ï¼ˆã‚ã‚Œã°ï¼‰
            chosen = (
                (data.get("response") or {}).get("chosen")
                or (data.get("result") or {}).get("chosen")
                or (data.get("decision") or {}).get("chosen")
                or data.get("chosen")
                or {}
            )
            unc = chosen.get("uncertainty", data.get("uncertainty", None))
            try:
                if unc is not None:
                    uncertainties.append(float(unc))
            except Exception:
                pass

        metrics[cat]["count"] += 1
        parsed += 1

    # âœ¨ TrustLog å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆæ–°æ©Ÿèƒ½ï¼‰
    trustlog_stats = analyze_trustlog()
    
    # æœ€çµ‚è¨ºæ–­æ™‚åˆ»ï¼ˆTrustLogã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°ç¾åœ¨æ™‚åˆ»ï¼‰
    last_check = trustlog_stats.get("created_at") or datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    avg_unc = round(statistics.mean(uncertainties), 3) if uncertainties else 0.0

    result = {
        "total_files_found": found_total,
        "parsed_logs":       parsed,
        "skipped_zero":      skipped_zero,
        "skipped_badjson":   skipped_bad,
        "avg_uncertainty":   avg_unc,
        "keywords":          keywords,
        "last_check":        last_check,
        "by_category":       {k: v["count"] for k, v in metrics.items()},
        "trustlog":          trustlog_stats,  # âœ¨ TrustLogçµ±è¨ˆã‚’è¿½åŠ 
        "generated_at":      datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "source_dir":        str(LOG_DIR),
    }

    os.makedirs(os.path.dirname(REPORT_PATH), exist_ok=True)
    with open(REPORT_PATH, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    # ---- console summary ------------------------------------------------
    print("\n== VERITAS Doctor Report (Enhanced) ==")
    print("âœ“ æ¤œå‡º(ç·):", found_total)
    print("âœ“ è§£æOK :", parsed)
    print("â†ª ã‚¹ã‚­ãƒƒãƒ—: 0B=", skipped_zero, ", JSON=", skipped_bad)
    print("ğŸ¯ å¹³å‡ä¸ç¢ºå®Ÿæ€§:", avg_unc)
    print("ğŸ”‘ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦:", keywords)
    print("ğŸ“… æœ€çµ‚è¨ºæ–­æ™‚åˆ»:", last_check)
    print("ğŸ“Š ã‚«ãƒ†ã‚´ãƒªå†…è¨³:", {k: v["count"] for k, v in metrics.items()})
    
    # âœ¨ TrustLogè¨ºæ–­çµæœã‚’è¡¨ç¤º
    print("\nğŸ”’ TrustLog è¨ºæ–­:")
    print(f"   ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {trustlog_stats['status']}")
    print(f"   ç·ã‚¨ãƒ³ãƒˆãƒªæ•°: {trustlog_stats['entries']}")
    
    if trustlog_stats['status'] == 'not_found':
        print("   âš ï¸ trust_log.jsonl ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    elif trustlog_stats['chain_valid']:
        print("   âœ… ãƒãƒƒã‚·ãƒ¥ãƒã‚§ãƒ¼ãƒ³æ¤œè¨¼: PASSED")
        if trustlog_stats['last_hash']:
            print(f"   ğŸ”‘ æœ€çµ‚ãƒãƒƒã‚·ãƒ¥: {trustlog_stats['last_hash']}")
    else:
        print("   âŒ ãƒãƒƒã‚·ãƒ¥ãƒã‚§ãƒ¼ãƒ³æ¤œè¨¼: FAILED")
        if trustlog_stats['chain_breaks'] > 0:
            print(f"   âš ï¸ ãƒã‚§ãƒ¼ãƒ³ç ´æ: {trustlog_stats['chain_breaks']} ç®‡æ‰€")
            if trustlog_stats['first_break']:
                fb = trustlog_stats['first_break']
                print(f"      æœ€åˆã®ç ´æ: Line {fb['line']} (ID: {fb['request_id']})")
        if trustlog_stats['hash_mismatches'] > 0:
            print(f"   âš ï¸ ãƒãƒƒã‚·ãƒ¥ä¸ä¸€è‡´: {trustlog_stats['hash_mismatches']} ä»¶")
            if trustlog_stats['first_mismatch']:
                fm = trustlog_stats['first_mismatch']
                print(f"      æœ€åˆã®ä¸ä¸€è‡´: Line {fm['line']} (ID: {fm['request_id']})")
    
    print("\nâœ… ä¿å­˜å®Œäº†:", REPORT_PATH)


if __name__ == "__main__":
    analyze_logs()

